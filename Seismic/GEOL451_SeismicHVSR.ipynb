{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Data Processing and HVSR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismology is one of the oldest and largest branches of geophysics. \n",
    "Seismic data is somewhat unique in its breadth of application: from planet-scale studies to hyper-local site characterization. In large part because of the global nature of seismic phenomena and because of the highly destructive potential of earthquakes, seismic data is highly organized and is shared globally. Countries develop their own seismometer networks, the data from which are often available to be downloaded or \"streamed\" online. Seismic data has many national security implications: it is used to detect nuclear tests, monitor border crossings, and explore for mineral or oil resources.\n",
    "\n",
    "Seismic data (as well as HVSR specifically, which we will focus on here) is also one of the only types of data collected on Earth, the Moon, and Mars.\n",
    "\n",
    "A basic understanding of seismic data, processing, and terminology is often expected in most roles having to do with geopysics.\n",
    "Environmental applications of seismic geophysical data are not as common as, for example, GPR and ERT.\n",
    "Understanding how to work with seismic data is still important for those working in environmental geophysics for a number of reason:\n",
    "* Seismic data and terminology can be used in many environmental applications\n",
    "* Many of the processing techniques used in GPR are directly analogous to (if not exactly the same as) techniques developed for seismology\n",
    "* Seismic data have many near-surface applications that will often overlap with environmental investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seismic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seismic data is at its core time-series data, and many of the processing techniques used in seismology are analagous to (if not exactly the same as) techniques used in and developed for broader signal processing algorithms.\n",
    "\n",
    "You should have an understanding of basic seismic terminology, but some terms worth emphasizing are included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General seismological terms:\n",
    "\n",
    "* **Seismometer**: an instrument that measures seismic data\n",
    "* **Seismic Network**: a collection of seismic stations, often managed by a single entity and with a singular purposes\n",
    "* **Seismic Station**: a seismometer (or multiple seismometers) that has been given a specific identifying code. Often, this is formatted as follows:\n",
    "    * NETWORK_NAME.STATION_NAME.LOCATION.CHANNEL\n",
    "* **Channel**: Often, the basic data feed or measurement coming out of a seismometer (or similar instrument). Many seismometer have more than one channel.\n",
    "    * These channels often consist of data read from a single geophone\n",
    "    * Among the most common configurations is a three-component seismometer: a vertical geophone, a horizontal geophone facing \"East\", and an orthogonal horizontal geophone facing \"north\"\n",
    "* **Geophone**: a device contained within a seismometer that measures ground motion\n",
    "    * The data from a geophone is often recorded as one channel in a seismometer's data record\n",
    "* **Component**: Often used interchangeably with \"channel\", but more specifically refers to directionality of the geophone\n",
    "    * The term **channel** implies a single set of data from a single geophone with consistent parameters\n",
    "\n",
    "### Terms or classes with a specific usage in Obspy\n",
    "* **Trace**: the basic building block of seismic data in Obspy\n",
    "    * A trace is a class in obspy that consists of a single stream of data and its associated metadata\n",
    "    * The time component of an obspy trace is always in UTC time\n",
    "    * Traces can have gaps in time, which can either be \"merged\" as a single trace with a \"masked\" array or \"split\" into multiple traces.\n",
    "    * Much of a trace's metadata is contained in its \"stats\" attribute\n",
    "* **Stream**: a collection of seismic data in Obspy, often the most basic data type read in from a real data source\n",
    "    * Streams consist of multiple traces. For example, the data from a three component seismometer ideally consists of three traces collected into in a single stream\n",
    "    * Most of the obspy functions or methods that work on streams actually perform on the individual traces in the stream.\n",
    "* **UTC**: \"Universal time coordinates,\" similar to an official scientific \"time zone\"\n",
    "    * For the most part, it is aligned with Greenwich Mean Time (i.e., the time in England)\n",
    "    * In obspy, these time coordinates are implented as `UTCDateTime` objects\n",
    "    * `UTCDateTime` is a class to standardize the time dimension of seismic data, but also has varous methods and attributes that allow manipulation from other common python time objects (such as objects in the Datetime module of the python standard library and matplotlib times for plotting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Data: Data in time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important aspects of seismic data is the time dimension. In fact, modern seismology depends almost entirely on the ability to accurately and precisely measure when ground motion occured. It is perhaps as important as the magnitude of the ground motion itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with time, we should first try understand the objects used for time in python.\n",
    "\n",
    "There was a relatively large change in how python deals with time natively that was released with python version 3.9, so please ensure you have version 3.9 or greater installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary time and date module in python's standard library is called `datetime`. The following is a non-comprehensive overview of python packages that deal with time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard library (installed with python itself)\n",
    "* **datetime**: The primary python module for creating and dealing with dates and times\n",
    "    * **date**: similar to and compatible with datetime, but only uses dates\n",
    "    * **time**: simliar to and compatible with datetime, but only deals with times\n",
    "    * **tzinfo**: module for creating timezone objects, converting between timezones, etc.\n",
    "        * **timezone**: class for working with timezones and offsets from UTC, for example\n",
    "    * **timedelta**: module for getting the difference between two datetime values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3rd-Party Libraries\n",
    "* **pytz**: installed as a dependency in pandas, pytz brings the [Olson (or IANA) timezone database](https://en.wikipedia.org/wiki/Tz_database) into Python\n",
    "* **tzdata**: a python data-only package that provides access to [Internet Assigned Numbers Authority (IANA) timezone database](https://www.iana.org/time-zones)\n",
    "* **matplotlib.pyplot.time**: submodule of matplotlib's pyplot that allows manipulation of time\n",
    "* **UTCDateTime**: a module of the obspy package, this is the primary object used to represent time in obspy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first familiarize ourselves with the datetime module.\n",
    "\n",
    "The datetime module contains several submodules. One is also called `datetime`. The `date` and `time` submodules are, to put it in a simple way, essentially one half each of the `datetime` class. (technically, `datetime` is a subclass of the `date` class)\n",
    "\n",
    "You can create a time object (at midnight: 00:00:00) with the following code. You can add arguments for hours, minutes, seconds, microseconds, and timezone information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Create a time object at midnight\n",
    "midnightTime = datetime.time()\n",
    "#this is the same as datetime.time(hour=0, minute=0, second=0, microsecond=0)\n",
    "print(midnightTime)\n",
    "oneSecond5MicrosAfterMidnight = datetime.time(hour=0, minute=0, second=1, microsecond=5)\n",
    "print(oneSecond5MicrosAfterMidnight)\n",
    "midnightTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that may be useful enough, but...which midnight do we mean (i.e., where on earth?). This is not specified by default with native datetime objects, but we can make these objects timezone-aware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zoneinfo\n",
    "usc = zoneinfo.ZoneInfo('US/Central') #\"US/Central\" is the IANA name for central time\n",
    "uscMidnight = datetime.time(tzinfo=usc)\n",
    "uscMidnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A printout of available timezones can be printed using the following code (this is a set of officially-recognized timezone names):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zoneinfo\n",
    "zoneinfo.available_timezones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually more that can be done on `datetime.datetime` objects with timezones. \n",
    "\n",
    "For example, let's say that we acquire data in the field using our local time (e.g., Central Time in the U.S.).\n",
    "\n",
    "However, our seismic data is likely to be in UTC. If we want to programatically \"translate\" this time, we can do so! Let's first define a `datetime` object in our local ('US/Central' in this case) timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zoneinfo\n",
    "# First, define the date and timezone\n",
    "usc = zoneinfo.ZoneInfo('US/Central')\n",
    "\n",
    "# specifying tzinfo makes datetimes timezone-aware\n",
    "oct102010=datetime.datetime(2010, 10, 10, 5, 10, tzinfo=usc)\n",
    "# same as: oct102010=datetime.datetime(year=2010, month=10, day=10, hour=5, minute=10, tzinfo=usc)\n",
    "\n",
    "print(oct102010)\n",
    "oct102010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert that timezone-aware datetime object to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct102010UTC = oct102010.astimezone(zoneinfo.ZoneInfo('UTC'))\n",
    "print(oct102010UTC)\n",
    "oct102010UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able (only relatively recently) to get `datetime` objects from the standard library specifically into UTC.\n",
    "\n",
    "Even so, python's native datetime module is not the most robust or intuitive for specifying times in UTC, which is very important for seismic data!\n",
    "\n",
    "So, the Obspy module has its own class for keeping track of datetimes to avoid this confusion (and to add useful functionality). This class is called `UTCDateTime`.\n",
    "\n",
    "For example, rather than microsecond precision out of the box, the `UTCDateTime` has nanosecond precision. There is also no confusion as to what timezone the time data is in, since it is always in UTC.\n",
    "\n",
    "`UTCDateTime` can be called similarly to the native python `datetime.datetime` class (i.e., by specifying year, month, day, etc.), but it also has many more options to maintain compatibility with a variety of seismic systems.\n",
    "\n",
    "For example, one of the more commonly used alternative date specifiers is called the \"Julian day\" which in this case essentially means the day of the year (e.g., Feb 1 would be the 32nd day of the year). The Julian Day can be used both for input and output of `UTCDateTime` objects.\n",
    "\n",
    "Obspy's `UTCDateTime` can easily determine this value, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "oct102010UTCDT = obspy.UTCDateTime(2010, 10, 10, 5, 10)\n",
    "print(oct102010UTCDT.julday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, practice on your own. We want to create a `UTCDateTime` object for the due date and time of this assignment (March 14, 2025 at 11:59pm and 59 seconds and  999,999 microseconds CENTRAL TIME).\n",
    "\n",
    "First, create a timezone-aware datetime object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import zoneinfo\n",
    "\n",
    "year = \n",
    "month =\n",
    "day =\n",
    "hour =\n",
    "minute=\n",
    "second=\n",
    "microsecond=999999\n",
    "\n",
    "centralDT = datetime.datetime(year, month, day, hour, minute, second, microsecond, tzinfo=zoneinfo.ZoneInfo(\"US/Central\"))\n",
    "obspy.UTCDateTime(centralDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, translate it to a timezone of your choice (can be your home timezone if that is different than \"US/Central\", UTC, or whatever you choose!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the time to a timezone of your choice!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: What time zone did you convert to (Use the official IANA name, e.g., from the \"available_timezones()\" command)? Copy the printout of the time as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say you were granted a 12-hour extension.\n",
    "\n",
    "To do this in the native python `datetime` module, you will need to use a `timedelta` object, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralDT + datetime.timedelta(hours=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done more directly with `UTCDateTime` objects. With UTCDateTime objects, you can use `+` and `-` to calculate changes in time. The default unit is seconds. So, use your answer from the previous cell (Q1) and add 12 hours to it (hint: how many seconds in an hour?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can read a timezone-aware datetime object into UTCDateTime fairly easily:\n",
    "centralDTUTC = obspy.UTCDateTime(centralDT)\n",
    "print(\"centralDT variable in UTC: \", centralDTUTC)\n",
    "\n",
    "twelve_hours_in_seconds = \n",
    "\n",
    "twelveHrExt = centralDTUTC + twelve_hours_in_seconds\n",
    "print(\"   with 12 hour extension: \", twelveHrExt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Copy/paste your UTCDateTime printout with the 12-hour-extension as your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Data: traces and streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"background-color: white\" src=\"https://docs.obspy.org/_images/Stream_Trace.png\" width=\"50%\" height=\"50%\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at how data is stored and organized in obspy. (see more about this [here](https://docs.obspy.org/packages/obspy.core.html) and the image below)\n",
    "\n",
    "You can read a short, sample dataset using obspy using `obspy.read()`. \n",
    "\n",
    "Normally, we would enter a filepath between the parentheses of the `read()` function. If we leave it blank, we will get a sample dataset.\n",
    "\n",
    "Read in that sample data stream below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "sampleStream = obspy.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different ways to see what our dataset looks like. One is to simply print the string representation of the obspy Stream object.\n",
    "\n",
    "This will give a heading with the number of traces in the stream, and a brief summary of each stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampleStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you iterate through a Stream object (using a `for` loop, for example), it will go through each Trace object contained in the Stream object.\n",
    "\n",
    "This looks similar to printing the Stream object, but it does not contain the \"header\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through traces\n",
    "for trace in sampleStream:\n",
    "    print('TRACE INFORMATION:', trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also simply plot a stream, which will produce an individual subplot for each trace.\n",
    "\n",
    "Setting it equal to a variable will save the matplotlib figure into that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplePlot = sampleStream.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to extract a specific trace from the stream, we have a few options.\n",
    "\n",
    "You can using indexing, just as if the traces were in a list. Note the data type this returns (i.e., an obspy Trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstTrace = sampleStream[0]\n",
    "zTrace = firstTrace # This just happens to be the Z trace\n",
    "print(type(zTrace))\n",
    "print(zTrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `.select()` method of the obspy streams to select all traces (only one in this instance) with a specific parameter.\n",
    "\n",
    "In this case, we'll select all traces in our stream from the \"EHE\" channel. (There can be multiple EHE channel traces if, for example, there is a large time gap in the data or if there are multiple stations' stream in the Stream object you are using). \n",
    "\n",
    "You can select using many attributes via the `.select()` method. In this case, we are selecting using the \"channel\" attribute.\n",
    "\n",
    "Note the data type that the `.select()` method returns (i.e., an obspy Stream). Even though there is only one Trace in this Stream, the `.select()` method returns a Stream. \n",
    "\n",
    "Later, we can use the indexing selection again to isolate a trace as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTraceStream = sampleStream.select(channel='EHE')\n",
    "print(type(eTraceStream))\n",
    "print(eTraceStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select using the component: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrace = sampleStream.select(component='N')\n",
    "print(type(nTrace))\n",
    "print(nTrace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to find the metadata attached to each Trace object.\n",
    "\n",
    "To do this, we'll use the `.stats` attribute of a Trace object. Note that Stream objects DO NOT have a `.stats` attribute. (see figure above)\n",
    "\n",
    "Run the next two cells to see what this looks like (the first one should come back with an error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTraceStream.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's extract the first (and only) trace from the Stream, then we can access the .stats\n",
    "eTrace = eTraceStream[0]\n",
    "eTrace.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different properties of the `.stats` attribute can be accessed using square bracket or dot notation. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eTrace.stats['starttime'])\n",
    "print(eTrace.stats.endtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these stats refer to properties of the instrument or data stream itself.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "Instrument:\n",
    "* **Network**: the seismic network of which the data is a part\n",
    "* **Station**: the station that recorded the data\n",
    "* **Location**: the location is a flexible term that can be used to differentiate between multiple stations at a single location, for example\n",
    "\n",
    "Data Stream:\n",
    "* **Channel**: the actual recording stream\n",
    "* **Starttime**: the starttime (in UTC) of this specific data object/stream\n",
    "* **Endtime**: the endtime (in UTC) of this specific data object/stream\n",
    "* **sampling_rate**: the number of data points/samples per second that make up the data\n",
    "* **delta**: the difference in time (in seconds) between each individual data point/sample\n",
    "* **npts**: the total number of data points/samples in the Trace\n",
    "* **response**: an obspy class containing a set of mathemetical parameters used to convert the electronic data (e.g, in volts) to physical motion (e.g., in millimeters) (see [here](https://docs.obspy.org/master/packages/autogen/obspy.core.inventory.response.Response.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to understand the mathematics or physics of seismic instrument response for this course, but it is good to be familiar with a few general concepts regarding instrument response. \n",
    "\n",
    "You can access the `Response` obspy object of the trace using the `.response` property of the `.stats` attribute and learn about how the instrument converts physical motion to digital form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eTrace.stats.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just look at instrument sensitivity as an example of what the response can show you. The sensitivity of the instrument is essentially the conversion factor between physical movement and digital bits and is defined by the sensitivity. This is usually defined at a frequency far enough above the [corner frequency](https://en.wikipedia.org/wiki/Cutoff_frequency) where the response curve is flat. \n",
    "\n",
    "You can view this curve (annotated) by using the `.plot()` method of `response` objects. You will need to set the `min_freq`. Use 0.001 Hz. More information on the `.plot()` method of the `Response` class [here](https://docs.obspy.org/master/packages/autogen/obspy.core.inventory.response.Response.plot.html#obspy.core.inventory.response.Response.plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response of your data using min_freq=0.001. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Include the plot of the instrument response of your eTrace variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Data: Global forces and global networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some other data manipulation with our seismic data.\n",
    "\n",
    "First we will read in some \"real\" seismic data. \n",
    "\n",
    "We will first practice by reading data from an online source.\n",
    "\n",
    "You can use the following code to find the available seismic data sources via the obspy `Client` class.\n",
    "\n",
    "These clients have access to different kinds of data. Some of these are earthquake catalogs, some have raw seismic data, and others have various seismic-related products. We will use the raw waveforms in this exercise.\n",
    "\n",
    "We will be using the IRIS database. You can find the service URL in the output of the cell below ([http://service.iris.edu](http://service.iris.edu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn.header import URL_MAPPINGS\n",
    "for key in sorted(URL_MAPPINGS.keys()):\n",
    "    print(\"{0:<11} {1}\".format(key,  URL_MAPPINGS[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see if we can find stations near the University of Illinois campus.\n",
    "\n",
    "We will use a seismic network called the [\"Transportable Array\"](http://www.usarray.org/researchers/obs/transportable). This is/was a network of seismometers spaced 70 km apart that were been temporarily installed for two years at a time at sites around the United States to better understand the lithospheric composition of North America. See [here](https://www.youtube.com/watch?v=h7aOPTyhsqs) for more information on the Transportable Array.\n",
    "\n",
    "This was sponsored by the National Science Foundation and deployed by the Incoporated Research Institutes on Seismology (IRIS), now called Earthscope.\n",
    "\n",
    "We will use Obspy's client class to access IRIS's data services to get data from this network and others to see a specific event in 2013 that originated from East Asia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event that we are interested in occured on February 12, 2013 at 2:57:51 UTC. Create a `UTCDateTime` object with that information in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "eventTime = UTCDateTime(year=, month=, day=, hour=, minute=, second=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create an instance of the client class for IRIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Client class instance for IRIS\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "\n",
    "\n",
    "irisClient = Client(\"IRIS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find all seismic stations part of the Transportable Array within a 0.5 degree radius of the University of Illinois that were active during February 2013.\n",
    "\n",
    "You only need to put year, month, and day in the `UTCDateTime` objects below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a starttime of Feb 1, 2013 and and endtime of March 1, 2013\n",
    "sTime = UTCDateTime()\n",
    "eTime = UTCDateTime()\n",
    "\n",
    "stationInventory = irisClient.get_stations(starttime=sTime, endtime=eTime, \n",
    "                        longitude=-88.2, latitude=40.1, maxradius=0.5,\n",
    "                        network=\"TA\")\n",
    "\n",
    "stationLats = []\n",
    "stationLons = []\n",
    "for network in stationInventory:\n",
    "    for station in network:\n",
    "        # Print out each station that fits the parameters above\n",
    "        print(station)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the information above to fill in the variables below to get raw waveform data (we have already set some of the parameters for you.)\n",
    "\n",
    "We will use data from the Mansfield station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "\n",
    "#Now, let's set our parameters\n",
    "# Info on get_waveforms() parameters: https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms.html#obspy.clients.fdsn.client.Client.get_waveforms\n",
    "# We need at minimum the network name, station name, location, channel, starttime, and endtime\n",
    "\n",
    "# Let's specify parameters for \n",
    "# You will need to fill in the network (\"TA\" for transportable array) and station from above\n",
    "# The event is only clear on the Mansfield seismometer, so set the sta variable equal to the four-character station code below\n",
    "net = \"TA\"\n",
    "sta = \n",
    "\n",
    "# These parameters are filled in for you already\n",
    "loc = '--' #This means a location value was not set for this station\n",
    "cha = 'BH*' # This says all channels that are B(roadband), H(igh gain), and *(any component)\n",
    "sTimeIL = eventTime + (60*60) # Start one hour after the event time\n",
    "eTimeIL = sTimeIL + (20*60) # Get a twenty minute record\n",
    "\n",
    "irisDataIL = irisClient.get_waveforms(network=net, \n",
    "                         station=sta, \n",
    "                         location=loc, \n",
    "                         channel=cha, \n",
    "                         starttime=sTimeIL, \n",
    "                         endtime=eTimeIL)\n",
    "\n",
    "print(irisDataIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform a simple bandpass filter on the data. This eliminates all data with frequencies lower than the `freqmin` parameter and higher than the `freqmax` parameter value.\n",
    "\n",
    "There are many kinds of frequency filters (these are also commonly used on GPR data, by the way). Common filters include:\n",
    "* **Bandpass:** frequencies between a minimum frequency and maximum frequency are retained; the rest of the data is removed\n",
    "* **Bandstop:** frequencies between a minimum frequency and maximum frequency are removed; the rest of the data is retained\n",
    "* **Highpass:** frequencies above a specified frequency are retained; the rest of the data is removed\n",
    "* **Lowpass:** frequencies below a specified frequency are retained; the rest of the data is removed\n",
    "\n",
    "For this, we will use the bandpass filter and set `freqmin` to 1 Hz and `freqmax` to 10 (data with frequency components between 1 and 10 Hz will be retained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will plot twenty minutes of data recorded in Illinois starting an hour after the event occured in East Asia \n",
    "\n",
    "irisDataIL.plot()\n",
    "irisDataIL_EDIT = irisDataIL.copy() #Filtering is done in-place on your data, so let's make a copy so we can keep our original data separate\n",
    "irisDataIL_EDIT.filter(\"bandpass\", freqmin=1, freqmax=10)\n",
    "plottedData = irisDataIL_EDIT.plot(method='full', linewidth=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at data from the same event, but much closer to it (from a seismometer in Japan).\n",
    "\n",
    "Note that we start our plot at the recorded event start time. \n",
    "\n",
    "The second plot below is in \"relative\" time, which means it is displayed as seconds after the starttime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "\n",
    "#Now, let's set our parameters\n",
    "# Info on get_waveforms() parameters: https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms.html#obspy.clients.fdsn.client.Client.get_waveforms\n",
    "# We need at minimum the network name, station name, location, channel, starttime, and endtime\n",
    "\n",
    "# Let's specify parameters for \n",
    "net='II'\n",
    "sta= \"ERM\"\n",
    "loc='00'\n",
    "cha='BH*'\n",
    "eTime = eventTime + 210 # 3.5- minute record\n",
    "\n",
    "\n",
    "irisDataJapan = irisClient.get_waveforms(network=net, \n",
    "                         station=sta, \n",
    "                         location=loc, \n",
    "                         channel=cha, \n",
    "                         starttime=eventTime, \n",
    "                         endtime=eTime)\n",
    "print(irisDataJapan)\n",
    "irisDataJapan.plot()\n",
    "irisDataJapan_EDIT = irisDataJapan.copy()\n",
    "irisDataJapan_EDIT.filter(\"bandpass\", freqmin=2.5, freqmax=9.9)\n",
    "\n",
    "# Code for plotting\n",
    "plottedData = irisDataJapan_EDIT.plot(type='relative', linewidth=0.2, show=False)\n",
    "ax = plottedData.get_axes()[2]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.sca(ax)\n",
    "import numpy as np\n",
    "ax.set_xticks(np.arange(0, 210, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's zoom in on the arrival time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's specify parameters for \n",
    "net='II'\n",
    "sta= \"ERM\"\n",
    "loc='00'\n",
    "cha='BH*'\n",
    "\n",
    "irisDataJapan = irisClient.get_waveforms(network=net, \n",
    "                                        station=sta, \n",
    "                                        location=loc, \n",
    "                                        channel=cha, \n",
    "                                        starttime=eventTime+140, \n",
    "                                        endtime=eventTime+195)\n",
    "\n",
    "print(irisDataJapan)\n",
    "irisDataJapan_EDIT = irisDataJapan.copy()\n",
    "irisDataJapan_EDIT.filter(\"bandpass\", freqmin=2.5, freqmax=9.9)\n",
    "\n",
    "# Code for plotting\n",
    "plottedData = irisDataJapan_EDIT.plot(type='relative', starttime=eventTime+145, linewidth=0.5, show=False)\n",
    "ax = plottedData.get_axes()[2]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.sca(ax)\n",
    "import numpy as np\n",
    "ax.set_xticks(np.arange(0, 50, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need the following information to answer Question 4c:\n",
    "\n",
    "For the ERM station (in the II network in Japan), note the following:\n",
    "* The BHZ channel is oriented vertically\n",
    "* The BH1 channel is oriented approximately North-South\n",
    "* The BH2 channel is oriented approximately East-West\n",
    "\n",
    "For the data stream:\n",
    "* In this last code cell, we have set an arbitrary start time so that we can \"zoom in\" on the arrival of the seismic waves to the II.ERM station.\n",
    "* There is a small initial set of waves that are visible between approximately 10-12 seconds into the record. These waves are oriented in the direction of travel (P waves).\n",
    "* There are other sets of waves that arrive around 12 and 14 seconds into the record.\n",
    "  * The exact timing of these waves would be important for precisely locating the source of the seismic data, but for now, we will just use approximations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: (Answer these using the last three code cells you have run, in order)\n",
    "# a) Given that the event occured on Feb 12, 2013 at 02:57:51 UTC, approximately how many minutes did it take to arrive to the seismometer in Illinois?\n",
    "# b) Approximately how many seconds after the event occured did the main waves arrive at the ERM station in Japan? (your answer should be within 10 seconds or so).\n",
    "# c) What differences do you see between BH1 and BH2 in the magnitude of the first set of waves (~10-12 seconds)? What does this imply about the direction of the source from the seismometer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: This seismic event was actually a nuclear bomb test undertaken by North Korea. \n",
    ">You can read more about it on the USGS Earthquake Database [here](https://earthquake.usgs.gov/earthquakes/eventpage/usc000f5t0/executive), Wikipedia [here](https://en.wikipedia.org/wiki/2013_North_Korean_nuclear_test), from NPR [here](https://www.npr.org/2013/02/12/171775235/north-korea-admits-it-carried-out-nuclear-test), CNN [here](https://www.cnn.com/2013/02/11/world/asia/north-korea-seismic-disturbance/index.html), and Al Jazeera [here](https://www.youtube.com/watch?v=RMdl9aa6ctI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismic Data: Local Site Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this exercise, you have carried out general overviews of seismic data in a way that is often used by geophysicists around the world. Because of the global (and large!) nature of seismic phenomena, seismic geophysics (i.e., seismology) is often carried out on global, continental, or at least regional scales.\n",
    "\n",
    "Seismic data acquisition at smaller scales is generally divided up into two types: active source and passive source.\n",
    "\n",
    "Some common methods used for these site-specific analyses are:\n",
    "* Active source surveys:\n",
    "    * Seismic reflection\n",
    "    * Seismic refraction\n",
    "    * (Multichannel) Analysis of Surface Waves (MASW) - more commonly an active source method\n",
    "* Passive source surveys (using [seismic noise](https://en.wikipedia.org/wiki/Seismic_noise)):\n",
    "    * Multichannel Analysis of Surface Waves (MASW) - may also be used with passive data\n",
    "    * Horizontal to Vertical Seismic Ratio (HVSR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this exercise, you will carry out HVSR analysis on passive seismic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data we will use for the last part of this exercise is included in the \"SeismicSampleData\" subfolder of the \"Seismic\" folder of the GEOL451 Github repository: `GEOL451/Seismic/SeismicSampleData`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HVSR Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HVSR data is commonly used to delineate the depth to subsurface interfaces where there are large differences in acoustic impedence. It does this by calculating the \"resonance frequency\" of our data, largely from the horizontally-polarized surface waves. This contrasts with a relative lack of resonance in the vertically-polarized surface waves. Ideally, we will find a \"fundamental frequency\" of the near-surface sediments/geologic formation where the horizontal waves resonate with a maximal amount of energy over the vertical waves. This creates a local maximum or \"peak\" in our H/V curve at a specific frequency.\n",
    "\n",
    "HVSR is one of the few direct geologic/geophysical techniques that has been carried out on Earth, the Moon, and on Mars. This is because it requires only a single, three-component seismometer \n",
    "(you could carry this out with a two component seismometer as well, as long as one is vertical and one is horizontal).\n",
    "\n",
    "HVSR is often used to delineate depth to bedrock in glaciated areas, or may be used for sounding the depth to other interfaces.\n",
    "\n",
    "We will use the python package \"Spectral Ratio Investigation Toolset\" or SpRIT (pronounced \"sprite\") to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrying out HVSR analysis with `sprit` can be done in a single line of code using the `sprit.run()` command, but we will go through each step individually here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general steps to carrying out HVSR analysis are:\n",
    "1. Read in parameters and fetch data\n",
    "2. **OPTIONAL**: Manipulate data to remove noise or generate azimuthal calculations\n",
    "    * Since we are using passive data, \"cultural\" noise (i.e., from sources near the seismometer) can intefere with analysis. \n",
    "        * We often remove the data for that time from the further analysis\n",
    "    * We may also rotate our data/calculate \"azimuthal\" seismic data to estimate the contribution of the H/V ratio from different directions\n",
    "3. Carry out power spectral density (PSD) analysis using fourier transforms (i.e., create PSD curves)\n",
    "    * Divide the data into (often overlapping) windows for analysis\n",
    "        * By default in SpRIT, these windows are 30 seconds in length and overlap by 50%\n",
    "    * Carry out fourier transforms on each component in each window to get a PSD curve for all windows/components\n",
    "4. Calculate the H/V ratio for each window\n",
    "    * For 3-component systems, this involves combining/averaging the PSD curve for the Horizontal components\n",
    "    * We divide the (combined) PSD curve value at each frequency step for the Horizontal component in each window by the PSD curve value at each frequency step for the Vertical component (H/V)\n",
    "5. Carry out post-processing on the H/V curve to identify key curve parameters\n",
    "    * **OPTIONAL**: Remove any outlier curves for simpler analysis\n",
    "    * Find the largest and/or most important \"peak\" in the curve (often, the fundamental frequency)\n",
    "    * Test that peak against standard tests (see section 3.2 and 3.3 of [this document](https://sesame.geopsy.org/Papers/HV_User_Guidelines.pdf) for the most commonly used tests)\n",
    "        * SESAME was a group that helped to codify and formalize HVSR analysis in the early 2000s\n",
    "6. Visualize/communicate results\n",
    "\n",
    "After collecting many HVSR soundings, you may also:\n",
    "\n",
    "7. Create a depth model to convert fundamental frequencies to depths\n",
    "8. Generate depth curves and subsurface profiles to visualize the soundings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SpRIT, these steps are implemented via the following functions, the names of which should be fairly self-explanatory:\n",
    "```python\n",
    "import sprit\n",
    "\n",
    "sprit.input_params()\n",
    "sprit.fetch_data()\n",
    "sprit.remove_noise()\n",
    "sprit.generate_psds()\n",
    "sprit.process_hvsr()\n",
    "sprit.remove_outlier_curves()\n",
    "sprit.check_peaks()\n",
    "sprit.get_report()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our process by setting up our `HVSRData` object in SpRIT, then by reading in the .mseed file from your SeismicSampleData folder.\n",
    "\n",
    "In the next few steps, we will find the following information:\n",
    "\n",
    "### The following information is in reference to our seismic data:\n",
    "* Filepath (you will need to specify this for the `input_data` parameter)\n",
    "* Site name (you may specify this (or not) however you like using the `site` parameter)\n",
    "    * You may also choose to specify the `project` parameter or not\n",
    "* Start time of record (`starttime` parameter): \n",
    "    * This will need to be an obspy `UTCDateTime` object\n",
    "* End time of record (`endtime` parameter): \n",
    "    * This will need to be an obspy `UTCDateTime` object\n",
    "\n",
    "### The following information is in reference to the location of the seismic data:\n",
    "* UTM Easting (`xcoord` parameter):\n",
    "* UTM Northing (`ycoord` parameter):\n",
    "    * You will need to specify the correct CRS for the `input_crs` parameter\n",
    "    * Using [epsg.io](epsg.io), you can search for the correct EPSG identifer by searching \"UTM 16N NAD83 2011\"\n",
    "    * You will want to set input_crs in this way: `input_crs=\"EPSG:####\"` (replace ### with the proper EPSG identifying number of the CRS)\n",
    "* Elevation (`elevation` parameter): \n",
    "    * If this is in feet, you either need to update the `elev_unit` parameter (`elev_unit='feet'`) or you need to convert this value to meters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, explore your data using obspy. Use the cell below:\n",
    "\n",
    "> **YOU MAY NEED TO UPDATE YOUR FILEPATH** (if you are not using Github Codespaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "inputData = \"/workspaces/GEOL451/Seismic/SeismicSampleData/GEOL451_HVSR_SampleData.mseed\"\n",
    "hvsrSeismicStream = obspy.read(inputData)\n",
    "print(hvsrSeismicStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the information from the printout above and any data you can extract from the `stats` property of the traces to fill in the rest of the data for the `sprit.input_params()` parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do any other data exploration of hvsrSeismicStream here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need xcoord, ycoord, input_crs, elevation, and elev_unit, and that is not contained in the data.\n",
    "\n",
    "Use the following parameters for `input_params` in the next code cell. You will go and find the surface elevation. You may use whatever method for getting elevation that you like, but you will need to explain your work in Q5 below.\n",
    "\n",
    "* ***xcoord***: 387189\n",
    "* ***ycoord***: 4463346\n",
    "* **input_crs**: \"EPSG:6345\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Provide the following information for the seismic data you will use for this HVSR point based on your data exploration in the previous code cell(s):\n",
    "(many of these you will use as parameters for the `input_params()` function of the SpRIT package)\n",
    "* **Network name** (two letter abbreviation) of the instrument (this will be your `network` parameter of `input_params`)\n",
    "* **Station name** of the instrument (this will be your `station` parameter )\n",
    "* **Names of the channels** (there should be three). \n",
    "    * Create a list and use it as your `channels` parameter (e.g., for the Japanese data above, it would be `channels=[\"BHZ\", \"BH1\", \"BH2\"]` (note the quotes))\n",
    "* **Acquisition date**\n",
    "    * This will be your `acq_date` parameter, as a `datetime.date` object (see [here](https://docs.python.org/3/library/datetime.html#datetime.date) for more info on `datetime.date` objects)\n",
    "* **Duration/length** of the data stream\n",
    "* **Sampling rate** (number of samples per second)\n",
    "* **Total number of samples** (per channel; all three should be the same)\n",
    "* **Elevation**: the surface elevation of the point, at the location given above (indicate unit)\n",
    "    * If this is in meters, you will need to assign \"m\" to the `elev_unit` parameter below. If feet, \"ft\" will work\n",
    "* **Coordinate Reference System**: provide the \"friendly name\" of the CRS identified by EPSG:6345, as well as the units of the CRS (degrees? Feet? meters?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you will need to specify your parameters. If you do this correctly, you should be able to also fetch your data with no issues. We will use the sample data that \"ships with\" the SpRIT package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sprit\n",
    "hvsrDataIN = sprit.input_params(input_data=inputData,\n",
    "                            #site=,\n",
    "                            #project=,\n",
    "                            network=,\n",
    "                            station=,\n",
    "                            channels=[],\n",
    "                            acq_date=,\n",
    "                            starttime=,\n",
    "                            endtime=,\n",
    "                            xcoord=,\n",
    "                            ycoord=,\n",
    "                            input_crs=,\n",
    "                            elevation=,\n",
    "                            elev_unit=,\n",
    "                            )\n",
    "hvsrDataIN = sprit.fetch_data(hvsrDataIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you process your data (in the next cell), you should get a plot with three subplots (similar to the plot below, though that is from a different dataset)\n",
    "\n",
    "The explanation of these subplots is below:\n",
    "* **H/V Ratio (top plot)**: A plot of the curve of the amplitude of the H/V ratio (Y axis) at each frequency step (x axis)\n",
    "    * The thick black line is the average H/V value at each frequency (as calculated over all (83 in this case) windows)\n",
    "    * The grey area is the standard deviation value of the H/V values at each frequency step.\n",
    "    * Dotted vertical line annotates the fundamental frequency\n",
    "* **Components PSDS (middle plot)**: The Power Spectral Densitry results calculated using a fourier transform for each window\n",
    "    * Black is Z/vertical component; Red is North component; Blue is East component\n",
    "    * Colored areas represent standard deviation for each component.\n",
    "    * Essentially, the red and blue are combined and divided by the black to get the top chart. \n",
    "        * When HVSR analysis is performing as expected, an \"eye\" shape appears in the components' PSD plot at the location of the fundamental frequency\n",
    "        * At this frequency, the horizontal components resonate, but the vertical does not\n",
    "* **H/V over time (bottom plot)**: The H/V ratio over time\n",
    "    * The amplitude of the H/V curve for each time window is mapped to a color (high amplitude=\"hot\" colors) and located on the x-axis according to the starttime of the window\n",
    "        * Stable H/V values shows as a 'hot' horizontal line across the chart (i.e., a consistent fundamental frequency over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"./SeismicSampleData/GEOL451_HVSRPlots.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go ahead and run the rest of the processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvsrData = sprit.generate_psds(hvsrDataIN)\n",
    "hvsrResults = sprit.process_hvsr(hvsrData)\n",
    "hvsrResults = sprit.check_peaks(hvsrResults)\n",
    "hvsrResults = sprit.get_report(hvsrResults, suppress_report_outputs=True)\n",
    "\n",
    "#You could also run all these steps in a single line of code, commented out below:\n",
    "#hvsrResults = sprit.run(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: Include a copy of just your plot OR a screenshot of the entire report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7: What did you calculate as fundamental/peak frequency of your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will derive a depth model to estimate the depth to this subsurface interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is commonly done by developing a regression relationship between known depths to the given subsurface interface and the fundamental frequencies.\n",
    "\n",
    "Usually, depth models vary depending on geology/region, so they are generated uniquely for each region or site that is being worked on.\n",
    "\n",
    "You will use example data from Champaign County, IL to generate a depth model for this region using the data provided in the SeismicSampleData folder in the `hvsr_depth_data.csv` file. \n",
    "\n",
    "See instructions below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at our data:\n",
    "\n",
    "> You may need to update the filepath if not working in Github codespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "depthDataPath = r\"/workspaces/GEOL451/Seismic/SeismicSampleData/GEOL451_HVSR_DepthModel.csv\"\n",
    "\n",
    "depthDataDF = pd.read_csv(depthDataPath, index_col='ID')\n",
    "depthDataDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 19 HVSR calibration points in this document, some with more/better information, some with less.\n",
    "\n",
    "That is ok, because for this analysis we only need a series of peak/fundamental frequencies and the depths (to bedrock likely) associated with that peak frequency.\n",
    "\n",
    "We will develop a model that uses the \"PeakFrequency\" column (fundamental frequency of an HVSR measurement, in Hz) as our indepenent variable and the \"BrDepthStn\" column (known depth (usually from a nearby well) to bedrock, in meters) as the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a simple plot of our data to visualize what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthDataDF.plot(x='PeakFrequency', y='BRDepthStn', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to generate a trendline/fit a model/create a regression.\n",
    "\n",
    "Excel has some nice tools for this. If you generate a chart, you can right click on the points in the series on the chart and create a trendline. Format the trendline so it is a \"power\" trendline, show the equation, and you can even show the $R^2$.\n",
    "\n",
    "We will use the native `sprit.calibrate()` function. This uses the `curve_fit` function in the `scipy.optimize` module to fit a line to a series of points.\n",
    "\n",
    "Run the cell below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_model = sprit.calibrate(depthDataPath, calib_depth_col=\"BRDepthStn\")\n",
    "depth_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has now generated two parameters for our depth model: a and b. The form of the equation that we will use is as follows:\n",
    "\n",
    "$$h = a * f_0^{b}$$\n",
    "\n",
    "Where:\n",
    "* h is the depth to the interface (bedrock most likely)\n",
    "* $f_0$ is the fundamental frequency obtained from our seismic data\n",
    "* a and b are the parameters from our depth model calibration.\n",
    "    * b is sometimes only used as a positive value, in which case the exponent would be -b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this formula to convert all the frequencies in our spectra to depths.\n",
    "\n",
    "Since you set the elevation in the `input_params` function, we can also subtract the depths from our surface elevation to get an elevation at the point at which the seismic data is resonating.\n",
    "\n",
    "We will generate a chart showing the H/V curve rotated on its side (in the depth/elevation domain instead of the frequency domain).\n",
    "\n",
    "Fill in the values of a and b below (you do not need to use all the decimal points...probably 5 significant digits is plenty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \n",
    "b = \n",
    "\n",
    "depthPlot = sprit.calculate_depth(hvsrResults, depth_model=(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8: Include the plot of the depth/elevation curve from the previous code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9: Include a sentence or two communicating any environmental implications your HVSR data may reveal about the site. (i.e., what kind of environmental information can we derive from knowing depth to bedrock). You may assume that groundwater does not flow easily across this bedrock interface."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
